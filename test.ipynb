{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import display\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image_array, image_number):\n",
    "    acrosome_labels = np.load(\"mhsma\\\\y_acrosome_train.npy\")\n",
    "    head_labels = np.load(\"mhsma\\\\y_head_train.npy\")\n",
    "    tail_labels = np.load(\"mhsma\\\\y_tail_train.npy\")\n",
    "    vacuole_labels = np.load(\"mhsma\\\\y_vacuole_train.npy\")\n",
    "\n",
    "    acrosome_label = acrosome_labels[image_number]\n",
    "    head_label = head_labels[image_number]\n",
    "    tail_label = tail_labels[image_number]\n",
    "    vacuole_label = vacuole_labels[image_number]\n",
    "    print(f\"Acrosome: {acrosome_label}, Head: {head_label}, Tail: {tail_label}, Vacuole: {vacuole_label}\")\n",
    "    plt.imshow(image_array, cmap='gray')\n",
    "    plt.axis('off') \n",
    "    plt.title(f\"Image {image_number}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.load('mhsma\\\\x_128_train.npy')\n",
    "x_test=np.load('mhsma\\\\x_128_test.npy')\n",
    "x_valid=np.load('mhsma\\\\x_128_valid.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = tf.cast(x_train, tf.float32)\n",
    "# x_test = tf.cast(x_test, tf.float32)\n",
    "# x_valid = tf.cast(x_valid, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.load('mhsma\\\\y_head_train.npy')\n",
    "y_test=np.load('mhsma\\\\y_head_test.npy')\n",
    "y_valid=np.load('mhsma\\\\y_head_valid.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = tf.cast(y_train, tf.float32)\n",
    "# y_test = tf.cast(y_test, tf.float32)\n",
    "# y_valid = tf.cast(y_valid, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=True,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=True,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rescale=1./255,\n",
    "    preprocessing_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_reshaped = x_train.reshape(1000, 128, 128, 1)\n",
    "x_valid_reshaped = x_valid.reshape(240, 128, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flow = datagen.flow(x_train_reshaped, y_train,batch_size=30)\n",
    "x_valid_flow = datagen.flow(x_valid_reshaped, y_valid,batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jayesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 255/255 [00:00<00:00, 85.1kB/s]\n",
      "c:\\Users\\Jayesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jayesh\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "image_processor  = AutoImageProcessor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jayesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "from transformers import SwinConfig, SwinModel\n",
    "configuration = SwinConfig()\n",
    "model = SwinModel(configuration)\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 71.8k/71.8k [00:00<00:00, 291kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 113M/113M [02:34<00:00, 733kB/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "model = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot assign value to variable ' conv2d/kernel:0': Shape mismatch.The variable shape (3, 3, 1, 32), and the assigned value shape (32, 3, 3, 3) are incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m basemodel\u001b[39m=\u001b[39mInceptionResNetV2(weights\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mimagenet\u001b[39;49m\u001b[39m'\u001b[39;49m,include_top\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, input_tensor\u001b[39m=\u001b[39;49mInput(shape\u001b[39m=\u001b[39;49m(\u001b[39m128\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m1\u001b[39;49m)))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\applications\\inception_resnet_v2.py:271\u001b[0m, in \u001b[0;36mInceptionResNetV2\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m         fname \u001b[39m=\u001b[39m (\n\u001b[0;32m    262\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minception_resnet_v2_weights_\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtf_dim_ordering_tf_kernels_notop.h5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m         )\n\u001b[0;32m    265\u001b[0m         weights_path \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39mget_file(\n\u001b[0;32m    266\u001b[0m             fname,\n\u001b[0;32m    267\u001b[0m             BASE_WEIGHT_URL \u001b[39m+\u001b[39m fname,\n\u001b[0;32m    268\u001b[0m             cache_subdir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    269\u001b[0m             file_hash\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39md19885ff4a710c122648d3b5c3b684e4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    270\u001b[0m         )\n\u001b[1;32m--> 271\u001b[0m     model\u001b[39m.\u001b[39;49mload_weights(weights_path)\n\u001b[0;32m    272\u001b[0m \u001b[39melif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m     model\u001b[39m.\u001b[39mload_weights(weights)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\backend.py:4360\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[1;34m(variable, value)\u001b[0m\n\u001b[0;32m   4357\u001b[0m     variable\u001b[39m.\u001b[39massign(d_value)\n\u001b[0;32m   4358\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4359\u001b[0m     \u001b[39m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[1;32m-> 4360\u001b[0m     variable\u001b[39m.\u001b[39;49massign(value)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot assign value to variable ' conv2d/kernel:0': Shape mismatch.The variable shape (3, 3, 1, 32), and the assigned value shape (32, 3, 3, 3) are incompatible."
     ]
    }
   ],
   "source": [
    "basemodel=InceptionResNetV2(weights='imagenet',include_top=False, input_tensor=Input(shape=(128, 128, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = basemodel.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x=Flatten()(x)\n",
    "x=Dense(256,activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x=Dense(128,activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(1, activation= 'sigmoid')(x)\n",
    "model = Model(inputs = basemodel.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer= SGD(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.input.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"rvgg16_1.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=34, verbose=1, mode='auto')\n",
    "hist = model.fit(steps_per_epoch=34,x=x_train_flow, validation_data= x_valid_flow,epochs=25,callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_file_path = 'mhsma\\\\x_128_train.npy'\n",
    "images = np.load(npy_file_path)\n",
    "image_number = 31\n",
    "display_image(images[image_number], image_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images[image_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_flip = cv2.flip(image, 1)\n",
    "vertical_flip = cv2.flip(image, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(horizontal_flip, image_number)\n",
    "display_image(vertical_flip, image_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_angle = 270\n",
    "rows, cols = image.shape\n",
    "rotation_matrix = cv2.getRotationMatrix2D((cols/2, rows/2), rotation_angle, 1)\n",
    "rotated_image = cv2.warpAffine(image, rotation_matrix, (cols, rows))\n",
    "\n",
    "display_image(rotated_image, image_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.8\n",
    "beta = 0\n",
    "contrast_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "\n",
    "display_image(contrast_image, image_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brightness_factor = 120\n",
    "brightness_image = cv2.add(image, np.full(image.shape, brightness_factor, dtype=np.uint8))\n",
    "\n",
    "display_image(brightness_image, image_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
